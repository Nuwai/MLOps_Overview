# ğŸ“˜ Chapter 9 â€” Cloud Deployment for ML (AWS, GCP, Azure)

Deploying ML models to the cloud is a core part of modern MLOps.  
This chapter covers real-world deployment patterns using:

- AWS EC2  
- AWS ECS  
- AWS Lambda (Serverless ML)  
- Google Cloud Run  
- Azure App Service  
- Nginx reverse proxy  
- Load balancing  
- Auto-scaling  
- Horizontal vs vertical scaling  

This is a **practical, production-ready guide** for ML cloud deployment.

---

# â˜ï¸ 9.1 Why Deploy Models to the Cloud?

Cloud deployment enables:

- Scalability  
- Reliability  
- High availability  
- Auto scaling  
- Secure access  
- CI/CD integration  
- GPU support  
- Cheaper, optimized compute  

Machine learning services must handle:
- Traffic spikes  
- Large payloads  
- Heavy models  
- Batch jobs  
- Real-time inference  

Cloud services provide infrastructure to manage all of these.

---

# ğŸ—ï¸ 9.2 Deployment Architecture Overview

```
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    FastAPI ML Service    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    Docker Container       â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Cloud Deployment (EC2/ECS/Run)â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Load Balancerâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ–¥ï¸ 9.3 Deploying to AWS EC2 (Virtual Machine)

This is the simplest cloud deployment method.

## **Steps:**

### 1ï¸âƒ£ Launch EC2 instance
- Choose Ubuntu or Amazon Linux  
- Assign public IP  
- Add inbound rule for port 80  

### 2ï¸âƒ£ Install Docker

```bash
sudo apt update
sudo apt install docker.io -y
sudo systemctl start docker
sudo systemctl enable docker
```

### 3ï¸âƒ£ Pull Docker image

```bash
docker pull myrepo/mlapi:latest
```

### 4ï¸âƒ£ Run container

```bash
docker run -d -p 80:80 myrepo/mlapi:latest
```

Your API is now live at:

```
http://EC2_PUBLIC_IP/
```

---

# ğŸ³ 9.4 Deploying to AWS ECS (Elastic Container Service)

ECS is a container orchestrator (simpler than Kubernetes).

## Architecture:

```
Docker Image â†’ ECR â†’ ECS Cluster â†’ Task â†’ Service â†’ Load Balancer
```

### Steps:

1. Push image to ECR  
2. Create ECS cluster  
3. Define Task Definition  
4. Define Service  
5. Attach Load Balancer  
6. Auto-scale tasks  

This setup is production-grade for APIs.

---

# âš¡ 9.5 Deploying ML Models with AWS Lambda (Serverless)

Lambda is **event-driven + serverless**.  
Good for **small ML models** or **light inference workloads**.

### Constraints:
- 15-minute max execution  
- Memory limit  
- CPU limit  
- Cold-start latency  

### When to use:
- Spam detection  
- Quick NLP classification  
- Image thumbnail generation  
- Event-based ML  

### Example Lambda Handler:

```python
import joblib

model = joblib.load("model.pkl")

def lambda_handler(event, context):
    data = event["body"]
    pred = model.predict([data])
    return {"prediction": pred[0]}
```

Package as a zip or container image.

---

# ğŸŒ 9.6 Deploying to Google Cloud Run (Recommended for ML)

Cloud Run is fully managed, serverless container hosting.

### Why ML engineers love it:
- Deploy Docker image  
- Auto scaling  
- Built-in HTTPS  
- Cheap (pay per request)  
- Supports GPUs (new feature)  
- No servers to manage  

### Deploy in one command:

```bash
gcloud run deploy mlapi \
  --image gcr.io/myproject/mlapi:latest \
  --platform managed
```

It returns a URL like:

```
https://mlapi-xyz.a.run.app
```

---

# ğŸŸ¦ 9.7 Deploy to Azure App Service

Steps:

1. Create App Service  
2. Enable Docker container mode  
3. Push image to Azure Container Registry  
4. Point App Service to image  
5. Configure scaling  

This is Azureâ€™s â€œCloud Runâ€.

---

# ğŸ”€ 9.8 Reverse Proxy with Nginx

Nginx handles:
- Routing  
- SSL  
- Load balancing  
- Rate limiting  
- Reverse proxy  

### Example `/etc/nginx/sites-available/mlapi.conf`

```nginx
server {
    listen 80;
    server_name example.com;

    location / {
        proxy_pass http://127.0.0.1:8000;
    }
}
```

Enable and restart:

```bash
sudo ln -s /etc/nginx/sites-available/mlapi.conf /etc/nginx/sites-enabled/
sudo systemctl restart nginx
```

---

# âš–ï¸ 9.9 Load Balancing

Load balancers distribute traffic across instances.

AWS:
- **ALB** (Application Load Balancer)

GCP:
- **Cloud Load Balancer**

Azure:
- **Application Gateway**

Improves:
- Availability  
- Performance  
- Fault tolerance  

---

# ğŸ“ˆ 9.10 Auto-Scaling ML Services

## Horizontal Scaling (recommended)
Add more instances.

```
Instance Ã— N
```

Works for:
- Many lightweight inference calls  
- Stateless ML APIs  

## Vertical Scaling
Add more CPU/GPU to one machine.

Use when:
- Model is large  
- GPU reliance  

---

# ğŸ—ï¸ 9.11 Real-World Cloud Architecture for ML

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Container Registry (ECR/AR) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ ML Deployment Target    â”‚
                          â”‚ (ECS / Cloud Run / App Service) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚    Load Balancer          â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚   Public API Endpoint   â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ§ª 9.12 Exercises

### **Exercise 1**
Deploy a Dockerized FastAPI ML service to EC2.

### **Exercise 2**
Deploy the same service to Cloud Run.

### **Exercise 3**
Add an Nginx reverse proxy to your EC2 deployment.

### **Exercise 4**
Set up auto-scaling on ECS or Cloud Run.

---

# ğŸ“ 9.13 Summary

- EC2 = simple VM hosting  
- ECS = scalable container orchestrator  
- Lambda = serverless ML  
- Cloud Run = easiest, cheapest managed platform  
- Azure App Service = container-based hosting  
- Use Nginx for reverse proxying  
- Load balancers ensure scalability  
- Auto-scaling ensures reliability  

Cloud deployment is a key MLOps capability â€” providing the foundation for reliable production ML.

---

# ğŸ‰ End of Chapter 9
