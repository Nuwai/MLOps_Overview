ğŸ“˜ Chapter 2 â€” ML Lifecycle Core Concepts
ğŸ§  2.1 Introduction

The Machine Learning lifecycle is the end-to-end process that turns raw data into a production-deployed model.
MLOps practices ensure the lifecycle is automated, reproducible, scalable, and observable.

This chapter covers:

Data collection

Data validation & quality

Feature engineering

Model training

Model evaluation

Drift

Retraining

Production ML architecture

ğŸ“ 2.2 Data Collection

Data collection is the foundation of every ML project.
Your model can only be as good as the quality and consistency of the data it receives.

Common Data Sources

SQL/NoSQL databases

Data warehouses (BigQuery, Snowflake)

Data lakes (S3, GCS, ADLS)

REST APIs & Webhooks

Event streams (Kafka)

Application logs

IoT and sensor data

User input or product telemetry

MLOps Requirements

Automation: ETL/ELT tools (Airflow, Prefect)

Versioning: Data snapshots stored via DVC or Delta Lake

Validation: Prevent bad data from entering pipelines

ASCII Diagram
            Raw Data Sources
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  DB   â”‚   APIs    â”‚   S3    â”‚
         â””â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚         â”‚
             â–¼         â–¼
      Automated Ingestion Pipeline

ğŸ“ 2.3 Data Quality

Bad data = bad predictions.
Data quality ensures ML systems produce accurate and reliable outputs.

Dimensions of Data Quality
Dimension	Meaning
Completeness	Missing values?
Consistency	Schema aligned?
Accuracy	Is it correct?
Freshness	Up-to-date?
Uniqueness	No duplicates?
Validity	In expected range?
Data Quality Problems

âŒ Incorrect labels

âŒ Missing or corrupted values

âŒ Inconsistent units (e.g., meters vs feet)

âŒ Duplicate data

âŒ Outliers

Tools

Great Expectations

TensorFlow Data Validation

EvidentlyAI

ğŸ“ 2.4 Feature Engineering

Feature engineering converts raw data into model-ready inputs.

Common Feature Engineering Techniques

Scaling/Normalization

One-hot encoding / Embeddings

Polynomial features

Aggregations (rolling averages, sums)

Domain-specific extractions

Feature selection (remove uninformative features)

Feature Store

A feature store ensures:

Same features in training & production

Centralized feature definitions

Low-latency retrieval for online inference

Examples:
Feast, Tecton, Vertex Feature Store.

ğŸ“ 2.5 Model Training

Model training is the process of learning patterns from data.

Training Workflow

Split dataset (train/validation/test)

Select model/algorithm

Tune hyperparameters

Run experiments

Version artifacts

Save trained model

Training in MLOps

Reproducibility via Docker

Automating jobs with orchestration tools

Tracking experiments using MLflow or Weights & Biases

Fully scriptable processes

ğŸ“ 2.6 Model Evaluation

Different problems require different evaluation metrics.

Classification Metrics

Accuracy

Precision

Recall

F1-score

ROC-AUC

Confusion Matrix

Regression Metrics

RMSE

MAE

RÂ²

Production Metrics

Latency

Throughput

Error rate

Fairness

Robustness

Evaluation Architecture
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Model Predictionsâ”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
            Compare with Ground Truth
                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  Evaluation  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ 2.7 Drift (Data, Concept, Model)

Drift is the silent killer of production ML systems.

Types of Drift
1. Data Drift

Input distributions change over time.
Example: shifts in user age demographics.

2. Concept Drift

Relationship between features and target changes.
Example: buying behavior before vs. after COVID.

3. Model Drift

Performance drops due to data or concept drift.

Drift Detection Tools

EvidentlyAI

WhyLabs

Arize AI

ğŸ“ 2.8 Retraining

Retraining ensures your model stays up-to-date.

Retraining Triggers

Drift detected

Performance degradation

New labeled data

Scheduled retraining

Retraining Pipeline
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    New Data    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
  Data Validation
        â–¼
 Feature Engineering
        â–¼
     Training
        â–¼
   Evaluation
        â–¼
 Model Registry
        â–¼
 Deployment (CI/CD)


Automated retraining is typically managed by:

Airflow

Kubeflow

Prefect

Argo Workflows

ğŸ“ 2.9 ML System Architecture (Modern ML Stack)

A real ML system contains far more than just a model.

Core Components
1ï¸âƒ£ Data Layer

Data ingestion

Data validation

Feature store

2ï¸âƒ£ Training Layer

Scalable training jobs

Experiment tracking

Model registry

3ï¸âƒ£ Deployment Layer

Model servers (FastAPI, TensorRT, TorchServe)

Containerization (Docker)

Orchestration (Kubernetes)

4ï¸âƒ£ Monitoring Layer

Metrics & logs

Drift detection

Alerting

Architecture Diagram
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Data Sources  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                 Ingestion / ETL / ELT
                           â–¼
                    Feature Store
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     Training Pipeline  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
                 Model Registry
                      â–¼
               Deployment Service
                      â–¼
                 Monitoring Layer
                      â–¼
                  Retraining Loop

ğŸ“ 2.10 Summary Table
Component	Purpose
Data collection	Automate & version data
Data quality	Validate and clean data
Feature engineering	Transform data for ML
Training	Reproducible model creation
Evaluation	Measure performance & fairness
Drift	Detect distribution changes
Retraining	Keep model fresh
Architecture	Production-ready ML system
