# ğŸ“˜ Chapter 7 â€” Model Registry & Versioning (MLflow, DVC, Experiments)

Model versioning is one of the **core pillars of MLOps**.  
Modern ML systems must track:

- Which **data** trained a model  
- Which **code** built it  
- Which **parameters** were used  
- Which **metrics** it achieved  
- Which **model artifact** was deployed  
- Which **version** is currently running in production  

This chapter explains how real companies track and manage models using **MLflow**, **DVC**, and **model registries**.

---

# ğŸ§  7.1 Why Model Versioning Matters

Machine learning is different from traditional software:

- Code changes â†’ different results  
- Data changes â†’ different results  
- Hyperparameters â†’ different results  
- Random seeds â†’ different results  

Without proper versioning, ML becomes **non-reproducible, unpredictable, and risky**.

### Real problems:
- â€œWhich model is running in production?â€ â†’ No answer  
- â€œWhy did accuracy drop?â€ â†’ No data version tracking  
- â€œCan we reproduce the last good model?â€ â†’ Impossible  

Model versioning solves all of these.

---

# ğŸ—‚ï¸ 7.2 What Is a Model Registry?

A **Model Registry** is a centralized system for:

- Registering models  
- Storing versioned model artifacts  
- Adding metadata (author, metrics, description)  
- Approving/rejecting models  
- Tracking production/staging/dev versions  
- Managing lifecycle events  

### Model lifecycle phases:

```
Saved â†’ Registered â†’ Staging â†’ Production â†’ Archived
```

---

# ğŸ§ª 7.3 MLflow Overview

MLflow is the most widely used open-source tool for:

- Experiment tracking  
- Model versioning  
- Model registry  
- Reproducible ML pipelines  

Install:

```bash
pip install mlflow
```

Start UI:

```bash
mlflow ui
```

Open in browser:

```
http://127.0.0.1:5000
```

---

# ğŸ·ï¸ 7.4 Experiment Tracking with MLflow

Example training script:

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier

mlflow.start_run():

    model = RandomForestClassifier(n_estimators=100)
    model.fit(X_train, y_train)

    acc = model.score(X_test, y_test)

    mlflow.log_param("n_estimators", 100)
    mlflow.log_metric("accuracy", acc)
    mlflow.sklearn.log_model(model, "model")

mlflow.end_run()
```

### MLflow automatically tracks:
- Parameters  
- Metrics  
- Artifacts (models, plots, files)  
- Git commit hash  
- Timestamp  

---

# ğŸ¯ 7.5 Model Registry with MLflow

Register a model:

```python
result = mlflow.register_model(
    "runs:/<run_id>/model",
    "churn_model"
)
```

Transition model to production:

```python
from mlflow.tracking import MlflowClient

client = MlflowClient()
client.transition_model_version_stage(
    name="churn_model",
    version=1,
    stage="Production"
)
```

---

# ğŸ“¦ 7.6 Loading Models from MLflow Registry

```python
import mlflow.pyfunc

model = mlflow.pyfunc.load_model("models:/churn_model/Production")
pred = model.predict({"age": 30, "income": 40000})
```

This allows **production services** (FastAPI, Lambda, Batch jobs) to load the latest model.

---

# ğŸ“ 7.7 DVC (Data Version Control)

DVC solves one critical problem:

> **Git cannot store large datasets or model artifacts.**

DVC provides:
- Data versioning  
- Model versioning  
- Pipelines  
- Storage (S3/GCS/local)  

Install:

```bash
pip install dvc
```

Initialize:

```bash
dvc init
```

---

# ğŸ§ª 7.8 Versioning Data with DVC

Track a dataset:

```bash
dvc add data/train.csv
```

This creates:
- `train.csv.dvc` file  
- Dataset stored in remote cache  

Add remote storage:

```bash
dvc remote add -d myremote s3://mybucket/data
```

Push:

```bash
dvc push
```

Anyone on your team can now:

```bash
git pull
dvc pull
```

And they will have **exactly the same dataset**.

---

# ğŸ§± 7.9 DVC Pipelines

Example pipeline definition:

```bash
dvc run -n train \
  -d data/train.csv \
  -d train.py \
  -o model.pkl \
  python train.py
```

Run entire pipeline:

```bash
dvc repro
```

---

# ğŸ†š 7.10 MLflow vs DVC â€” Which to Use?

| Feature | MLflow | DVC |
|--------|--------|------|
| Experiment tracking | âœ… | âŒ |
| Model registry | âœ… | âŒ |
| Data versioning | âŒ | âœ… |
| Pipeline orchestration | âŒ | âœ… |
| Cloud storage | âœ“ | âœ“ |
| Reproducibility | âœ“ | âœ“ |

**Best practice in real companies:**

> Use MLflow for model tracking + registry,  
> Use DVC for dataset versioning.

---

# ğŸ”¢ 7.11 Versioning Strategies for ML

## 1ï¸âƒ£ Model Versioning
Example:

```
churn_model:v1
churn_model:v2
```

## 2ï¸âƒ£ Semantic Versioning

```
v1.0.0
v1.1.0
v2.0.0
```

## 3ï¸âƒ£ Git Hash Versioning

```
model_4f8d11a.pkl
```

## 4ï¸âƒ£ Dataset Versioning (DVC)

```
data@v4
data@v5
```

---

# ğŸ—„ï¸ 7.12 What to Store Along with a Model?

A model alone is **not enough**.  
You must store:

- Model artifact (pickle, ONNX, TorchScript)  
- Model signature  
- Parameters  
- Metrics  
- Environment (conda, pip freeze, Dockerfile)  
- Code version  
- Dataset version  
- Training timestamp  
- Author / owner  

This enables **total reproducibility**.

---

# ğŸ§ª 7.13 Example: Complete MLflow Training & Registry Script

```python
import mlflow
import mlflow.sklearn
from sklearn.linear_model import LogisticRegression

mlflow.set_experiment("churn")

with mlflow.start_run():

    model = LogisticRegression()
    model.fit(X_train, y_train)

    acc = model.score(X_test, y_test)

    mlflow.log_metric("accuracy", acc)
    mlflow.sklearn.log_model(model, artifact_path="model")

    model_info = mlflow.register_model(
        "runs:/{run_id}/model",
        "churn_model"
    )
```

---

# ğŸ§° 7.14 Real-World Architecture for Model Registry

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Training Jobs   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
            Experiment Tracking
                    â–¼
            Model Registry (MLflow)
                    â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼                           â–¼
Staging Environment         Production API
      â”‚                           â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Monitoring â—„â”€â”€â”˜
```

---

# ğŸš¨ 7.15 Common Mistakes

âŒ Storing models only on local machines  
âŒ No versioning for datasets  
âŒ No tracking for hyperparameters  
âŒ Manually naming model files  
âŒ Deploying models without registry  
âŒ No record of which model version is in production  

---

# ğŸ§ª 7.16 Exercises

### **Exercise 1**
Use MLflow to track:
- Two parameters  
- One metric  
- One model

### **Exercise 2**
Register a model and promote it to staging.

### **Exercise 3**
Initialize DVC and track `data.csv`.

### **Exercise 4**
Explain the difference between MLflow and DVC.

---

# ğŸ“ 7.17 Summary

- Model versioning is essential for reproducibility  
- MLflow handles experiment tracking & registry  
- DVC handles dataset + pipeline versioning  
- A good model registry stores metrics, artifacts, and metadata  
- Versioning strategies ensure traceability  

---

# ğŸ‰ End of Chapter 7
